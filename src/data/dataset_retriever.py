import os
import zipfile
import gdown
import json
import numpy as np
from PIL import Image
import io
from tqdm import tqdm
import shutil
import threading
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from multiprocessing import Pool, cpu_count, Manager, Lock
from typing import Tuple, List, Optional

class DatasetRetriever:
    """
    Classe pour tÃ©lÃ©charger, extraire et prÃ©parer le dataset Ã  partir d'une source donnÃ©e.
    """

    def __init__(self, config: dict, dataset_type: str = 'training'):
        """
        Initialise le DatasetRetriever avec la configuration.

        Arguments:
        - config (dict): Configuration contenant les paramÃ¨tres du dataset
        - dataset_type (str): 'training' ou 'inference' pour choisir le dataset
        """
        dataset_config = config['data'][f'{dataset_type}_dataset']
        self.gdrive_url = dataset_config['gdrive_url']
        self.target_images = dataset_config['target_images']
        
        # Utiliser les chemins de la configuration
        self.dataset_dir = os.path.join(config['paths']['dataset_dir'], dataset_type)
        
        # Chemins et structures de rÃ©pertoires
        self.master_zip_path = os.path.join(self.dataset_dir, 'master.zip')
        self.cbz_extract_path = os.path.join(self.dataset_dir, 'cbz_files')
        self.paths = {
            'bw': os.path.join(self.dataset_dir, 'source', 'bw'),
            'color': os.path.join(self.dataset_dir, 'source', 'color'),
            'metadata': os.path.join(self.dataset_dir, 'metadata'),
            'temp': os.path.join(self.dataset_dir, 'temp_cbz')
        }

        # CrÃ©ation des rÃ©pertoires nÃ©cessaires
        for path in self.paths.values():
            os.makedirs(path, exist_ok=True)

        # Variables pour le traitement
        self.image_counter = 0
        self.stop_processing = False
        self.num_workers = multiprocessing.cpu_count()
        self.num_processes = cpu_count()

    def download_and_extract(self) -> bool:
        """
        TÃ©lÃ©charge et extrait le fichier zip contenant les donnÃ©es.
        VÃ©rifie d'abord si les donnÃ©es sont dÃ©jÃ  extraites.

        Retourne:
            bool: True si le tÃ©lÃ©chargement et l'extraction ont rÃ©ussi, False sinon.
        """
        # VÃ©rifier si les donnÃ©es sont dÃ©jÃ  extraites
        if os.path.exists(self.cbz_extract_path) and os.listdir(self.cbz_extract_path):
            print("ğŸ“‚ Les fichiers CBZ sont dÃ©jÃ  extraits")
            return True

        # VÃ©rification si le fichier zip existe dÃ©jÃ 
        if os.path.exists(self.master_zip_path):
            zip_size = os.path.getsize(self.master_zip_path) / (1024 * 1024)  # Taille en MB
            print(f"ğŸ“¦ Le fichier master.zip existe dÃ©jÃ  ({zip_size:.2f} MB)")
            
            # VÃ©rifier si le fichier n'est pas corrompu
            try:
                with zipfile.ZipFile(self.master_zip_path, 'r') as zip_ref:
                    zip_ref.testzip()
                print("âœ… Le fichier zip est valide")
            except Exception as e:
                print(f"âš ï¸  Le fichier zip est corrompu, nouveau tÃ©lÃ©chargement nÃ©cessaire: {str(e)}")
                os.remove(self.master_zip_path)
                return self.download_and_extract()
        else:
            print("ğŸ“¥ TÃ©lÃ©chargement du master zip...")
            try:
                gdown.download(url=self.gdrive_url, output=self.master_zip_path, fuzzy=True)
            except Exception as e:
                print(f"âŒ Erreur lors du tÃ©lÃ©chargement: {str(e)}")
                return False

        if not os.path.exists(self.master_zip_path):
            print("âŒ Ã‰chec du tÃ©lÃ©chargement")
            return False

        print(f"âœ… Fichier zip prÃªt: {os.path.getsize(self.master_zip_path) / (1024 * 1024):.2f} MB")

        print("\nğŸ“¦ Extraction des fichiers CBZ...")
        os.makedirs(self.cbz_extract_path, exist_ok=True)

        try:
            with zipfile.ZipFile(self.master_zip_path, 'r') as zip_ref:
                zip_ref.extractall(self.cbz_extract_path)
            print("âœ… Extraction terminÃ©e")
            # Supprimer le zip aprÃ¨s extraction rÃ©ussie
            os.remove(self.master_zip_path)
            return True

        except Exception as e:
            print(f"âŒ Erreur lors de l'extraction: {str(e)}")
            return False

    def validate_image(self, img: Image.Image) -> Tuple[bool, str]:
        """
        Valide la qualitÃ© et les caractÃ©ristiques de l'image.

        Arguments:
        - img (Image.Image): Image PIL Ã  valider.

        Retourne:
        - is_valid (bool): True si l'image est valide, False sinon.
        - message (str): Message dÃ©crivant le rÃ©sultat de la validation.
        """
        if img.size[0] < 300 or img.size[1] < 300:
            return False, "Image trop petite"

        # VÃ©rifier le contraste et la nettetÃ©
        img_array = np.array(img)
        if img_array.std() < 20:  # VÃ©rification basique du contraste
            return False, "Contraste insuffisant"

        return True, "OK"

    def process_single_image(self, args: Tuple[str, str]):
        """
        Traite une seule image en effectuant la validation et la conversion.

        Arguments:
        - args (Tuple[str, str]): Tuple contenant le chemin de l'image et le rÃ©pertoire temporaire.
        """
        if self.stop_processing:
            return

        image_path, temp_dir = args
        try:
            img = Image.open(image_path)

            # Validation de l'image
            is_valid, message = self.validate_image(img)
            if not is_valid:
                print(f"Image ignorÃ©e ({image_path}): {message}")
                return

            with self.counter_lock:
                if self.image_counter >= self.target_images:
                    self.stop_processing = True
                    return
                image_id = f"image_{self.image_counter:05d}"
                self.image_counter += 1
                with self.total_lock:
                    self.total_images_processed.value += 1
                    self.pbar.update(1)

            # MÃ©tadonnÃ©es de l'image
            metadata = {
                'original_size': img.size,
                'original_mode': img.mode,
                'source_path': os.path.relpath(image_path, temp_dir),
                'chapter': os.path.basename(os.path.dirname(image_path)),
                'creation_date': os.path.getctime(image_path),
                'validation_message': message
            }

            # Sauvegarder l'image couleur
            color_path = os.path.join(self.paths['color'], f"{image_id}.png")
            img.save(color_path, 'PNG')

            # Convertir et sauvegarder en noir et blanc
            img_gray = img.convert('L')
            gray_path = os.path.join(self.paths['bw'], f"{image_id}.png")
            img_gray.save(gray_path, 'PNG')

            # Sauvegarder les mÃ©tadonnÃ©es
            metadata_path = os.path.join(self.paths['metadata'], f"{image_id}.json")
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=4)

        except Exception as e:
            print(f"Erreur lors du traitement de l'image {image_path}: {str(e)}")

    def get_filtered_images(self, directory: str) -> List[Tuple[str, str]]:
        """
        RÃ©cupÃ¨re les images filtrÃ©es en excluant les pages de couverture et de fin.

        Arguments:
        - directory (str): RÃ©pertoire contenant les images extraites.

        Retourne:
        - filtered_images (List[Tuple[str, str]]): Liste de tuples avec le chemin de l'image et le rÃ©pertoire temporaire.
        """
        chapters = {}
        skip_start = 6
        skip_end = 6

        for root, _, files in os.walk(directory):
            chapter_name = os.path.basename(root)
            if files:
                image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                if image_files:
                    image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x)) or 0))
                    chapters[chapter_name] = [os.path.join(root, f) for f in image_files]

        filtered_images = []
        chapter_names = sorted(chapters.keys())

        for i, chapter in enumerate(chapter_names):
            images = chapters[chapter]
            if i == 0 or i == len(chapter_names) - 1:
                images = images[skip_start:-skip_end]
            filtered_images.extend((img, directory) for img in images)

        return filtered_images

    def extract_cbz_wrapper(self, cbz_file: str) -> Optional[str]:
        """
        Wrapper pour l'extraction des fichiers CBZ, utilisÃ© pour le multiprocessing.

        Arguments:
        - cbz_file (str): Chemin vers le fichier CBZ Ã  extraire.

        Retourne:
        - temp_dir (Optional[str]): Chemin vers le rÃ©pertoire temporaire oÃ¹ les fichiers sont extraits.
        """
        if self.stop_processing:
            return None

        temp_dir = os.path.join(self.paths['temp'], os.path.splitext(os.path.basename(cbz_file))[0])
        os.makedirs(temp_dir, exist_ok=True)

        try:
            print(f"ğŸ“š Extraction de {os.path.basename(cbz_file)}...")
            with zipfile.ZipFile(cbz_file, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            return temp_dir
        except Exception as e:
            print(f"âŒ Erreur lors de l'extraction de {cbz_file}: {str(e)}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return None

    def create_dataset(self) -> int:
        """
        CrÃ©e le dataset en tÃ©lÃ©chargeant, extrayant et traitant les images.

        Retourne:
        - total_images_processed (int): Nombre total d'images traitÃ©es.
        """
        print(f"\nCrÃ©ation d'un dataset avec {self.target_images} images...")
        print(f"Utilisation de {self.num_processes} processus pour l'extraction")

        # Initialiser les variables de suivi ici
        self.counter_lock = threading.Lock()
        self.manager = Manager()
        self.total_images_processed = self.manager.Value('i', 0)
        self.total_lock = self.manager.Lock()
        self.pbar = tqdm(total=self.target_images, desc="Images traitÃ©es")

        cbz_files = [os.path.join(self.cbz_extract_path, f)
                     for f in os.listdir(self.cbz_extract_path)
                     if f.lower().endswith('.cbz')]

        # Utiliser une approche sÃ©quentielle pour l'extraction des CBZ
        temp_dirs = []
        for cbz_file in tqdm(cbz_files, desc="Extraction des tomes"):
            temp_dir = self.extract_cbz_wrapper(cbz_file)
            if temp_dir:
                temp_dirs.append(temp_dir)

        # Traitement des images
        for temp_dir in tqdm(temp_dirs, desc="Traitement des tomes"):
            if self.stop_processing:
                break

            try:
                image_files = self.get_filtered_images(temp_dir)
                with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                    list(executor.map(self.process_single_image, image_files))
            finally:
                shutil.rmtree(temp_dir, ignore_errors=True)

        if self.pbar:
            self.pbar.close()

        # Sauvegarde des mÃ©tadonnÃ©es
        total_processed = self.total_images_processed.value if hasattr(self.total_images_processed, 'value') else self.total_images_processed
        
        dataset_metadata = {
            'total_images': total_processed,
            'creation_date': os.path.getctime(self.dataset_dir),
            'structure_version': '2.0',
            'paths': {k: os.path.relpath(v, self.dataset_dir) for k, v in self.paths.items()}
        }

        with open(os.path.join(self.dataset_dir, 'dataset_info.json'), 'w') as f:
            json.dump(dataset_metadata, f, indent=4)

        print(f"\nâœ… Traitement terminÃ© !")
        print(f"Nombre total d'images traitÃ©es : {total_processed}")
        print(f"Dataset crÃ©Ã© dans : {self.dataset_dir}")

        # Nettoyage final
        shutil.rmtree(self.cbz_extract_path, ignore_errors=True)
        shutil.rmtree(self.paths['temp'], ignore_errors=True)

        return total_processed

    def prepare_dataset(self) -> int:
        """
        PrÃ©pare le dataset complet en tÃ©lÃ©chargeant et en traitant les donnÃ©es.

        Retourne:
        - total_images_processed (int): Nombre total d'images traitÃ©es.
        """
        print("ğŸš€ DÃ©marrage du processus...")

        if not self.download_and_extract():
            print("âŒ Ã‰chec de la prÃ©paration des fichiers")
            return 0

        print("\nğŸ¨ DÃ©marrage de la crÃ©ation du dataset...")
        return self.create_dataset()