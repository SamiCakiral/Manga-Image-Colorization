import tensorflow as tf
import os

def detect_hardware():
    """
    D√©tecte et configure le hardware disponible (CPU/GPU/TPU)
    Retourne la strat√©gie appropri√©e et un r√©sum√© du hardware
    """
    print("\nüîç D√©tection du hardware...")

    # Essai de d√©tection TPU
    try:
        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
        tf.config.experimental_connect_to_cluster(resolver)
        tf.tpu.experimental.initialize_tpu_system(resolver)
        strategy = tf.distribute.TPUStrategy(resolver)
        print("‚úì TPU d√©tect√© et initialis√©:")
        print(f"  - Nombre de TPUs: {strategy.num_replicas_in_sync}")
        return strategy, "TPU"
    except:
        print("  Pas de TPU d√©tect√©, recherche de GPU...")

    # D√©tection GPU
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            # Configuration de la m√©moire GPU
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)

            # Configuration multi-GPU si disponible
            strategy = tf.distribute.MirroredStrategy()

            # Informations d√©taill√©es sur les GPUs
            print("‚úì GPU(s) d√©tect√©(s) et configur√©(s):")
            print(f"  - Nombre de GPUs: {strategy.num_replicas_in_sync}")
            for gpu in gpus:
                desc = tf.config.experimental.get_device_details(gpu)
                if desc:
                    print(f"  - {gpu.device_type}: {desc.get('device_name', gpu.name)}")
                    if 'compute_capability' in desc:
                        print(f"    Compute capability: {desc['compute_capability'][0]}.{desc['compute_capability'][1]}")
                else:
                    print(f"  - {gpu.device_type}: {gpu.name}")

            # Informations sur la m√©moire GPU si disponible
            try:
                memory_info = []
                for gpu in gpus:
                    memory_limit = tf.config.experimental.get_memory_info(f'/device:GPU:{gpus.index(gpu)}')
                    total_memory = memory_limit['current'] / (1024**3)  # Conversion en GB
                    memory_info.append(f"    M√©moire disponible: {total_memory:.2f} GB")
                if memory_info:
                    print("\n  Informations m√©moire:")
                    print("\n".join(memory_info))
            except:
                pass

            return strategy, "GPU"

        except RuntimeError as e:
            print(f"‚ö†Ô∏è Erreur lors de la configuration GPU: {e}")
            print("  Utilisation du CPU √† la place")
            strategy = tf.distribute.get_strategy()
            return strategy, "CPU"
    else:
        print("  Pas de GPU d√©tect√©, utilisation du CPU")
        strategy = tf.distribute.get_strategy()
        return strategy, "CPU"

def print_hardware_info():
    """
    Affiche des informations d√©taill√©es sur le hardware et l'environnement
    """
    print("\nüíª Configuration syst√®me:")
    print(f"  - TensorFlow version: {tf.__version__}")
    print(f"  - Python version: {os.sys.version.split()[0]}")

    # Informations sur le CPU
    try:
        import cpuinfo
        cpu_info = cpuinfo.get_cpu_info()
        print(f"  - CPU: {cpu_info['brand_raw']}")
        print(f"  - Nombre de c≈ìurs CPU: {cpu_info['count']}")
    except:
        print("  - Impossible d'obtenir les informations d√©taill√©es du CPU")

    # Informations sur la m√©moire syst√®me
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"  - M√©moire syst√®me totale: {memory.total / (1024**3):.1f} GB")
        print(f"  - M√©moire syst√®me disponible: {memory.available / (1024**3):.1f} GB")
    except:
        print("  - Impossible d'obtenir les informations de m√©moire syst√®me")

def setup_hardware():
    """
    Configure le hardware et retourne la strat√©gie appropri√©e
    """
    print("\nüöÄ Configuration du hardware d'acc√©l√©ration...")

    # Activation du XLA JIT si possible
    if os.environ.get('XLA_FLAGS') is None:
        os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'
    tf.config.optimizer.set_jit(True)
    print("‚úì XLA JIT activ√©")

    # Configuration de la pr√©cision mixte
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    print("‚úì Pr√©cision mixte activ√©e (float16/float32)")

    # D√©tection et configuration du hardware
    strategy, hardware_type = detect_hardware()

    # Affichage des informations syst√®me
    print_hardware_info()

    return strategy, hardware_type

