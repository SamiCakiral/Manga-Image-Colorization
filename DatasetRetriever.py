import os
import zipfile
import gdown
import json
import numpy as np
from PIL import Image
import io
from tqdm import tqdm
import shutil
import threading
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from multiprocessing import Pool, cpu_count, Manager, Lock

class DatasetRetriever:
    """
    Classe pour tÃ©lÃ©charger, extraire et prÃ©parer le dataset Ã  partir d'une source donnÃ©e.
    """

    def __init__(self, gdrive_url: str, target_images: int, dataset_dir: str = '/content/dataset/'):
        """
        Initialise le DatasetRetriever avec l'URL de Google Drive et le nombre d'images cibles.

        Arguments:
        - gdrive_url (str): URL de tÃ©lÃ©chargement du fichier zip contenant les donnÃ©es.
        - target_images (int): Nombre d'images Ã  traiter pour le dataset.
        - dataset_dir (str): Chemin vers le rÃ©pertoire oÃ¹ le dataset sera stockÃ©.
        """
        self.gdrive_url = gdrive_url
        self.target_images = target_images
        self.dataset_dir = dataset_dir

        # Chemins et structures de rÃ©pertoires
        self.master_zip_path = os.path.join(self.dataset_dir, 'master.zip')
        self.cbz_extract_path = os.path.join(self.dataset_dir, 'cbz_files')
        self.paths = {
            'bw': os.path.join(self.dataset_dir, 'source', 'bw'),
            'color': os.path.join(self.dataset_dir, 'source', 'color'),
            'metadata': os.path.join(self.dataset_dir, 'metadata'),
            'temp': os.path.join(self.dataset_dir, 'temp_cbz')
        }

        # CrÃ©ation des rÃ©pertoires nÃ©cessaires
        for path in self.paths.values():
            os.makedirs(path, exist_ok=True)

        # Variables pour le traitement
        self.image_counter = 0
        self.counter_lock = threading.Lock()
        self.stop_processing = False
        self.num_workers = multiprocessing.cpu_count()
        self.manager = Manager()
        self.total_images_processed = self.manager.Value('i', 0)
        self.total_lock = self.manager.Lock()
        self.pbar = tqdm(total=target_images, desc="Images traitÃ©es")

        # Configuration du traitement des images
        self.target_size = (1024, 1024)
        self.quality_threshold = 50  # Seuil minimal de qualitÃ© (KB)
        self.num_processes = cpu_count()  # Nombre de processus pour l'extraction

    def download_and_extract(self) -> bool:
        """
        TÃ©lÃ©charge et extrait le fichier zip contenant les donnÃ©es.

        Retourne:
        - success (bool): True si le tÃ©lÃ©chargement et l'extraction ont rÃ©ussi, False sinon.
        """
        # VÃ©rification si le fichier existe dÃ©jÃ 
        if os.path.exists(self.master_zip_path):
            print("ğŸ“¦ Le fichier master.zip existe dÃ©jÃ ")
            print(f"Taille du fichier: {os.path.getsize(self.master_zip_path) / 1024 / 1024:.2f} MB")
        else:
            print("ğŸ“¥ TÃ©lÃ©chargement du master zip...")
            try:
                gdown.download(url=self.gdrive_url, output=self.master_zip_path, fuzzy=True)
            except Exception as e:
                print(f"âŒ Erreur lors du tÃ©lÃ©chargement: {str(e)}")
                return False

        if not os.path.exists(self.master_zip_path):
            print("âŒ Ã‰chec du tÃ©lÃ©chargement")
            return False

        print(f"âœ… TÃ©lÃ©chargement terminÃ©: {os.path.getsize(self.master_zip_path) / 1024 / 1024:.2f} MB")

        print("\nğŸ“¦ Extraction des fichiers CBZ...")
        os.makedirs(self.cbz_extract_path, exist_ok=True)

        try:
            with zipfile.ZipFile(self.master_zip_path, 'r') as zip_ref:
                zip_ref.extractall(self.cbz_extract_path)
            print("âœ… Extraction terminÃ©e")
            os.remove(self.master_zip_path)
            return True

        except Exception as e:
            print(f"âŒ Erreur lors de l'extraction: {str(e)}")
            return False

    def validate_image(self, img: Image.Image) -> Tuple[bool, str]:
        """
        Valide la qualitÃ© et les caractÃ©ristiques de l'image.

        Arguments:
        - img (Image.Image): Image PIL Ã  valider.

        Retourne:
        - is_valid (bool): True si l'image est valide, False sinon.
        - message (str): Message dÃ©crivant le rÃ©sultat de la validation.
        """
        if img.size[0] < 300 or img.size[1] < 300:
            return False, "Image trop petite"

        # VÃ©rifier le contraste et la nettetÃ©
        img_array = np.array(img)
        if img_array.std() < 20:  # VÃ©rification basique du contraste
            return False, "Contraste insuffisant"

        return True, "OK"

    def process_single_image(self, args: Tuple[str, str]):
        """
        Traite une seule image en effectuant la validation et la conversion.

        Arguments:
        - args (Tuple[str, str]): Tuple contenant le chemin de l'image et le rÃ©pertoire temporaire.
        """
        if self.stop_processing:
            return

        image_path, temp_dir = args
        try:
            img = Image.open(image_path)

            # Validation de l'image
            is_valid, message = self.validate_image(img)
            if not is_valid:
                print(f"Image ignorÃ©e ({image_path}): {message}")
                return

            with self.counter_lock:
                if self.image_counter >= self.target_images:
                    self.stop_processing = True
                    return
                image_id = f"image_{self.image_counter:05d}"
                self.image_counter += 1
                with self.total_lock:
                    self.total_images_processed.value += 1
                    self.pbar.update(1)

            # MÃ©tadonnÃ©es de l'image
            metadata = {
                'original_size': img.size,
                'original_mode': img.mode,
                'source_path': os.path.relpath(image_path, temp_dir),
                'chapter': os.path.basename(os.path.dirname(image_path)),
                'creation_date': os.path.getctime(image_path),
                'validation_message': message
            }

            # Sauvegarder l'image couleur
            color_path = os.path.join(self.paths['color'], f"{image_id}.png")
            img.save(color_path, 'PNG')

            # Convertir et sauvegarder en noir et blanc
            img_gray = img.convert('L')
            gray_path = os.path.join(self.paths['bw'], f"{image_id}.png")
            img_gray.save(gray_path, 'PNG')

            # Sauvegarder les mÃ©tadonnÃ©es
            metadata_path = os.path.join(self.paths['metadata'], f"{image_id}.json")
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=4)

        except Exception as e:
            print(f"Erreur lors du traitement de l'image {image_path}: {str(e)}")

    def get_filtered_images(self, directory: str) -> List[Tuple[str, str]]:
        """
        RÃ©cupÃ¨re les images filtrÃ©es en excluant les pages de couverture et de fin.

        Arguments:
        - directory (str): RÃ©pertoire contenant les images extraites.

        Retourne:
        - filtered_images (List[Tuple[str, str]]): Liste de tuples avec le chemin de l'image et le rÃ©pertoire temporaire.
        """
        chapters = {}
        skip_start = 6
        skip_end = 6

        for root, _, files in os.walk(directory):
            chapter_name = os.path.basename(root)
            if files:
                image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                if image_files:
                    image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x)) or 0))
                    chapters[chapter_name] = [os.path.join(root, f) for f in image_files]

        filtered_images = []
        chapter_names = sorted(chapters.keys())

        for i, chapter in enumerate(chapter_names):
            images = chapters[chapter]
            if i == 0 or i == len(chapter_names) - 1:
                images = images[skip_start:-skip_end]
            filtered_images.extend((img, directory) for img in images)

        return filtered_images

    def extract_cbz_wrapper(self, cbz_file: str) -> Optional[str]:
        """
        Wrapper pour l'extraction des fichiers CBZ, utilisÃ© pour le multiprocessing.

        Arguments:
        - cbz_file (str): Chemin vers le fichier CBZ Ã  extraire.

        Retourne:
        - temp_dir (Optional[str]): Chemin vers le rÃ©pertoire temporaire oÃ¹ les fichiers sont extraits.
        """
        if self.stop_processing:
            return None

        temp_dir = os.path.join(self.paths['temp'], os.path.splitext(os.path.basename(cbz_file))[0])
        os.makedirs(temp_dir, exist_ok=True)

        try:
            print(f"ğŸ“š Extraction de {os.path.basename(cbz_file)}...")
            with zipfile.ZipFile(cbz_file, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            return temp_dir
        except Exception as e:
            print(f"âŒ Erreur lors de l'extraction de {cbz_file}: {str(e)}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return None

    def create_dataset(self) -> int:
        """
        CrÃ©e le dataset en tÃ©lÃ©chargeant, extrayant et traitant les images.

        Retourne:
        - total_images_processed (int): Nombre total d'images traitÃ©es.
        """
        print(f"\nCrÃ©ation d'un dataset avec {self.target_images} images...")
        print(f"Utilisation de {self.num_processes} processus pour l'extraction")

        cbz_files = [os.path.join(self.cbz_extract_path, f)
                     for f in os.listdir(self.cbz_extract_path)
                     if f.lower().endswith('.cbz')]

        with Pool(processes=self.num_processes) as pool:
            temp_dirs = list(tqdm(
                pool.imap(self.extract_cbz_wrapper, cbz_files),
                total=len(cbz_files),
                desc="Extraction des tomes"
            ))

        temp_dirs = [d for d in temp_dirs if d is not None]

        for temp_dir in tqdm(temp_dirs, desc="Traitement des tomes"):
            if self.stop_processing:
                break

            try:
                image_files = self.get_filtered_images(temp_dir)
                with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                    list(executor.map(self.process_single_image, image_files))
            finally:
                shutil.rmtree(temp_dir, ignore_errors=True)

        self.pbar.close()

        # Sauvegarde des mÃ©tadonnÃ©es du dataset
        dataset_metadata = {
            'total_images': self.total_images_processed.value,
            'creation_date': os.path.getctime(self.dataset_dir),
            'structure_version': '2.0',
            'paths': {k: os.path.relpath(v, self.dataset_dir) for k, v in self.paths.items()}
        }

        with open(os.path.join(self.dataset_dir, 'dataset_info.json'), 'w') as f:
            json.dump(dataset_metadata, f, indent=4)

        print(f"\nâœ… Traitement terminÃ© !")
        print(f"Nombre total d'images traitÃ©es : {self.total_images_processed.value}")
        print(f"Dataset crÃ©Ã© dans : {self.dataset_dir}")

        # Nettoyage final
        shutil.rmtree(self.cbz_extract_path, ignore_errors=True)
        shutil.rmtree(self.paths['temp'], ignore_errors=True)

        return self.total_images_processed.value

    def prepare_dataset(self) -> int:
        """
        PrÃ©pare le dataset complet en tÃ©lÃ©chargeant et en traitant les donnÃ©es.

        Retourne:
        - total_images_processed (int): Nombre total d'images traitÃ©es.
        """
        print("ğŸš€ DÃ©marrage du processus...")

        if not self.download_and_extract():
            print("âŒ Ã‰chec de la prÃ©paration des fichiers")
            return 0

        print("\nğŸ¨ DÃ©marrage de la crÃ©ation du dataset...")
        return self.create_dataset()